<br>

<i>
This program was contributed by Thomas C Clevenger and Timo Heister.
<br>
This material is based upon work partly supported by the National
Science Foundation Award DMS-1901529, OAC-2015848, EAR-1925575, by the Computational
Infrastructure in Geodynamics initiative (CIG), through the NSF under Award
EAR-0949446 and EAR-1550901 and The University of California -- Davis.
</i>


@note As a prerequisite of this program, you need to have both p4est and either the PETSc
or Trilinos library installed. The installation of deal.II together with these additional
libraries is described in the <a href="../../readme.html" target="body">README</a> file.


<a name="Intro"></a>
<h1>Introduction</h1>


This example shows the usage of the multilevel functions in deal.II on distributed meshes
and gives a comparison between geometric and algebraic multigrid methods. The algebraic
multigrid (AMG) preconditioner is the same used in step-40, and the geometric multigrid
(GMG) preconditioner is based on the one used in step-16. Here we discuss the
necessary changes needed for parallel computations.


<h3>The testcase</h3>

We consider the variable-coefficient Laplacian weak formulation
@f{align*}
 (\epsilon \nabla u, \nabla v) = (f,v) \quad \forall v \in V_h
@f}
on the domain $\Omega = [-1,1]^\text{dim} \setminus [0,1]^\text{dim}$ (an L-shaped domain for 2D and a Fichera corner for 3D) with
$\epsilon = 1$ if $\min(x,y,z)>-\frac{1}{2}$ and $\epsilon = 100$ otherwise. The
boundary conditions are $u=0$ on the whole boundary and the right-hand side is $f=1$.
We use continuous Q2 elements to discretize $V_h$ and use a residual-based, cell-wise a
posteriori error estimator $e(K) = e_{\text{cell}}(K) + e_{\text{face}}(K)$ from
_CITE EST PAPER_ with
@f{align*}
 e_{\text{cell}}(K) = h^2 \| f + \epsilon \triangle u \|_K^2, \qquad
 e_{\text{face}}(K) = \sum_F h_F \| [ \epsilon \nabla u \cdot n ] \|_F^2.
@f}
The following figure visualizes the solution and refinement for 2D
<img src="https://www.dealii.org/images/steps/developer/step-50-2d-solution.png" alt="">
and for 3D, the solution(left) and a slice for $x$ close to the
center of the domain showing the adaptively refined mesh (right) are depicted here
<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-50-3d-solution.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-50-refinement.png" alt="">
    </td>
  </tr>
</table>


<h3>Workload imbalance</h3>
For the active mesh, we use the parallel::distributed::Triangulation class as done
in step-40 which uses functionality in the external library
<a href="http://www.p4est.org/">p4est</a> for the distribution of the active cells
among processors. For the non-active cells in the multilevel hierarchy, deal.II
implements what we will refer to as the ``first-child rule'' where, for each cell
in the hierarchy, we recursively assign the parent of a cell to the owner of the
first child cell. The following figures give an example of such a distribution. Here
the left image represents the active cells for a sample 2D mesh partitioned using a
space-filling curve (similar to p4est), the center image gives the tree representation
of the active mesh, and the right image gives the multilevel hierarchy of cells. The
colors and numbers represent the different processors. The circular nodes in the tree
are the non-active cells which are distributed using the ``first-child rule''.

<img src="https://www.dealii.org/images/steps/developer/step-50-workload-example.png" alt="">

Included among the output to screen in this example is a value ``Workload imbalance''
given by the function MGTools::workload_imbalance(). This value, which will be denoted
by $\mathbb{E}$,  quantifies the overhead produced by not having a perfect work balance
on each level of the multigrid hierarchy (as is evident from the example above).

For defining $\mathbb{E}$, let $N_{\ell}$ be the number of cells on level $\ell$
(both active and non-active cells) and $N_{\ell,p}$ of the subset owned by processor
$p$. Assuming that the workload for any one processor is proportional to the number
of cells owned by that processor, the optimal workload per processor is given by
@f{align*}
W_{\text{opt}}=\frac1{n_{p}}\sum_{\ell}\sum_{p}N_{\ell,p}=\frac1{n_{p}}\sum_{\ell} N_{\ell}.
@f}
Next, assuming a synchronization of work on each level (i.e., on each level of a vcycle,
work must be completed by all processors before moving on to the next level), the
limiting effort on each level is given by
@f{align*}
W_\ell = \max_{p} N_{\ell,p},
@f}
and the total parallel complexity
@f{align*}
W = \sum_{\ell} W_\ell.
@f}
Then we define $\mathbb{E}$ as a ratio of the optimal partition to the parallel
complexity of the current partition
@f{align*}
  \mathbb{E} = \frac{W_{\text{opt}}}{W}.
@f}
For the example distribution above, we have
@f{align*}
W_{\text{opt}}&=\frac{1}{n_p}\sum_{\ell} N_{\ell} = \frac{1}{3} \left(1+4+4\right)= 3 \qquad
\\
W &= \sum_\ell W_\ell = 1 + 2 + 3 = 6
\\
\mathbb{E} &= \frac{W_{\text{opt}}}{W} = \frac12.
@f}

_CITE MG PAPER_ contains a full discussion of the partition efficiency model
and the effect the imbalance has on the GMG vcycle timing. In summary, the value
of $\mathbb{E}$ is highly dependent on the type a mesh refinement used and has
optimal value $\mathbb{E} = 1$ for globally refined meshes. Typically for adaptively
refined meshes, the number of processors used to distribute a single mesh has a
negative impact on $\mathbb{E}$ but only up to a leveling off point, where the imbalance
remains relatively constant for an increasing number of processors, and further refinement
has very little impact on $\mathbb{E}$. Finally, $1/\mathbb{E}$ was shown to give an
accurate representation of the slowdown in parallel scaling expected for the timing of
a vcycle.
